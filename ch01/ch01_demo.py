# -*- coding: utf-8 -*-
"""ch01_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17o17hzAVKCXyAKQDLyro9H0DPknaZxPN
"""

import numpy as np

x = np.array([1,2,3])
print(x.__class__)
print("="*20)
print(x.shape)
print("="*20)
print(x.ndim)

"""Numpy 행렬의 원소별 연산 (Element-Wise Multiplication)

  실제 행렬간의 곱셈 연산을 위해선 행렬의 차원이 맞아야함.
  예를 들어  두 행렬 A와 B의 곱셈 C= AB가 성립하려면  A의 열의 수와 B의 행의 수가 일치해야함

  아래의 예제를 실제 행렬 곱셉을 위해 전치행렬을 사용한다면 결과물의 차원과 결과물의 의미가 달라질 수 있으므로 조심


"""

W=np.array([[1,2,3],[4,5,6]])
print(W.shape)
print(W.ndim)

X = np.array([[0,1,2],[3,4,5]])
print(W+X)
print("="*15)
print(W*X)

"""넘파이 차원의 Broadcast"""

A= np.array([[1,2],[3,4]])
print(A)
print("="*15)
print(A*10)

b=np.array([10,20])
print(b)
print("="*15)
print(b.shape)
print("="*15)
print(A*b)

"""벡터의 내적과 행렬의 곱

  백터의 내적은 직관적으로 '두 벡터가 얼마나 같은 방향을 향하고 있는가'
"""

a = np.array([1,2,3])
b = np.array([4,5,6])
np.dot(a,b)

A= np.array([[1,2],[3,4]])
B= np.array([[5,6],[7,8]])
np.matmul(A,B)

"""# 신경망의 추론

h1 = x1w11 + x2w21 + b1
---
"""

W1 = np.random.randn(2,4)
print(f"W1 \n {W1}")
print("="*30)
b1=np.random.randn(4)
print(f"b1 \n {b1}")
print("="*30)
x = np.random.randn(10,2)
print(f"x \n {x}")
print("="*30)
h = np.matmul(x,W1)+ b1
print(f"h \n {h}")
print("="*30)

"""## 시그모이드 함수

Fully Connect Layer에 의한 변환은 '선형'변환.

여기에 '비선형'효과를 부여하는 것이 바로 활성화 함수

σ(x) = 1 / 1 + exp(-1)
"""

def sigmoid(x) :
  return 1 / (1 + np.exp(-x))

a = sigmoid(h)
print(a)

"""전체 코드"""

import numpy as np

def sigmoid(x) :
  return 1 / (1 + np.exp(-x))

x = np.random.randn(10,2)
print(f"x \n {x}")
print("="*30)
W1 = np.random.randn(2,4)
print(f"W1 \n {W1}")
print("="*30)
b1=np.random.randn(4)
print(f"b1 \n {b1}")
print("="*30)
W2 = np.random.randn(4,3)
print(f"W2 \n {W2}")
print("="*30)
b2=np.random.randn(3)
print(f"b2 \n {b2}")
print("="*30)

h = np.matmul(x,W1)+ b1
a = sigmoid(h)
print(f"a \n {a}")
print("="*30)
s=np.matmul(a,W2)+b2
print(f"s \n {s}")
print("="*30)

"""# 계층으로 클래스화 및 순전파 구현

## 앞서 구현한 완전 연결 계층 (Fully Connected Layer)를 Affine으로,
## 시드모이드 함수에 의한 변환을 Sigmoid 변환
"""

import numpy as np

class Sigmoid:
  def __init__(self):
    self.params = []

  def forward(self,x):
    return 1 / (1 + np.exp(-x))

class Affine:
  def __init__(self,W,b):
    self.params = [W,b]

  def forward(self,x):
    W,b = self.params
    out = np.matmul(x,W) + b
    return out

class TwoLayerNet:
  def __init__  (self,input_size,hidden_size,output_size):
    I, H, O = input_size, hidden_size, output_size

    #가중치와 편향 초기화

    W1 = np.random.randn(I,H)
    b1 = np.random.randn(H)
    W2 = np.random.randn(H,O)
    b2 = np.random.randn(O)

    #계층 생성

    self.layers = [
                   Affine(W1,b1),
                   Sigmoid(),
                   Affine(W2,b2)
    ]

    #모든 가중치를 리스트에 모은다
    self.params = []
    for layer in self.layers:
      self.params += layer.params

  def predict(self,x):
    for layer in self.layers:
      x = layer.forward(x)
    return x

x=np.random.randn(10,2)
model = TwoLayerNet(2,4,3)
s = model.predict(x)
print(s)

"""## 1.3.1 손실함수

신경망 학습에서는 학습이 얼마나 잘 되고 있는 지를 알고 위한 '척도'가 필요

학습의 특정 단계에서 신경망의 성능을 나타내는 척도로 손실(loss)로 사용

"""

## 여기에 그림

"""## 1.3.4 계산 그래프


*   덧셈 노드
*   곱셈 노드
*   분기 노드
*   Repeat 노드
*   Sum 노드
*   Matmul 노드

## 1.3.5 기울기 도출과 역전파 구현
"""

class Sigmoid:
  def __init__(self):
    self.params, self.grads = [], []
    self.out = None

  def forward(self,x):
    out = 1 / (1 + np.exp(-x))
    self.out = out
    return out

  def backward(self,dout):
    dx = dout * (1.0 - self.out) * self.out
    return dx

class Affine:
  def __init__(self,W,b):
    self.params = [W,b]
    self.grads = [np.zeros_like(W),np.zeros_like(b)]
    self.x = None

  def forward(self, x):
    w,b = self.params
    out = np.matmul(x,w) + b
    self.x = x
    return out

  def backward(self,dout):
    w,b = self.params
    dx = np.matmul(dout,w.T)
    dw = np.matmul(self.x.T,dout)
    db = np.sum(dout,axis=0)

    self.grads[0][...] = dw
    self.grads[1][...] = db
    return dx

"""## softmax with Loss 계층

### 가중치 갱신


1.  미니 배치
2.  기울기 계산
3.  매개변수 갱신
4.  반복
"""

class SoftmaxWithLoss:
    def __init__(self):
        self.params, self.grads = [], []
        self.y = None  # softmax의 출력
        self.t = None  # 정답 레이블

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)

        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환
        if self.t.size == self.y.size:
            self.t = self.t.argmax(axis=1)

        loss = cross_entropy_error(self.y, self.t)
        return loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]

        dx = self.y.copy()
        dx[np.arange(batch_size), self.t] -= 1
        dx *= dout
        dx = dx / batch_size

        return dx

class SGD:
  def __init__(self,lr=0.01):
    self.lr = lr

  def update(self,params,grads):
    for i in range(len(params)):
      params[i] -= self.lr * grads[i]